{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T1. Introducción_a_PyTorch.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-IM5Xsebv0_n"},"source":["# Introducción a PyTorch\n","\n","**Curso:** CC227 - Introduction to Deep Learning\n","\n","**Autor:** Jhosimar George Arias Figueroa\n","\n","*Universidad Peruana de Ciencias Aplicadas (UPC)*\n","\n","-------"]},{"cell_type":"markdown","metadata":{"id":"QyJw28SGBX3A"},"source":["<p align=\"center\">\n","  <img src = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQWyV5brHNCcNRsfWARFMR78N0Wg5V2hksMVYQeOpYnBn4zKOX2PPWky9wqxVoXuvnUi_s&usqp=CAU\" width = \"300px\">\n","<p/>\n","\n","\n","¡Bienvenidos al tutorial de PyTorch para el curso de Deep Learning (CC227) de la Universidad Peruana de Ciencias Aplicadas (UPC)! El siguiente notebook está destinado a brindar una breve introducción a los conceptos básicos de PyTorch y a configurarlo para escribir sus propias redes neuronales. PyTorch es un framework de machine learning de código abierto que nos permite implementar nuestras propias redes neuronales y optimizarlas de manera eficiente. Sin embargo, PyTorch no es el único framework de este tipo. Las alternativas a PyTorch incluyen [TensorFlow](https://www.tensorflow.org/), [JAX](https://github.com/google/jax#quickstart-colab-in-the-cloud), [Caffe](http://caffe.berkeleyvision.org/), y muchos otros más. Se decidió enseñar PyTorch porque el framework está bien establecido, tiene una gran comunidad de desarrolladores (originalmente desarrollado por Facebook), es muy flexible y se usa especialmente en la investigación. Muchos artículos actuales publican su código en PyTorch, por lo tanto, es bueno estar familiarizado con este framework. Mientras tanto, TensorFlow (desarrollado por Google) generalmente es conocido por ser una biblioteca de deep learning aplicado mayormente en producción. Aún así, si aprendemos un framework de machine learning en profundidad, es muy fácil aprender otro porque muchos de ellos usan los mismos conceptos e ideas. Por ejemplo, la versión 2 de TensorFlow se inspiró en gran medida en las funciones más populares de PyTorch, lo que hace que los frameworks sean aún más similares.\n","\n","Hay muchos tutoriales excelentes en línea, incluido [\"60-min blitz\"](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) en el sitio web oficial de PyTorch. Sin embargo, este tutorial está diseñado para brindarle los conceptos básicos particularmente necesarios para las tareas. Durante las próximas semanas, también seguiremos explorando nuevas funciones de PyTorch aplicados a deep learning.\n","\n","Usaremos un conjunto de bibliotecas estándar que se utilizan a menudo en proyectos de machine learning. Si está ejecutando este notebook en Google Colab, todas las bibliotecas deben estar preinstaladas. Si está ejecutando este notebook localmente, asegúrese de haber instalado [PyTorch](https://pytorch.org/get-started/locally/) adecuadamente. "]},{"cell_type":"code","metadata":{"id":"92dkg5prBYKb"},"source":["## Bibliotecas estándar\n","import numpy as np \n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2xD0cKFcz1z2"},"source":["# Fundamentos de PyTorch\n","\n","Comenzaremos revisando los conceptos básicos de PyTorch. Como requisito previo, es recomendado estar familiarizado con el paquete `numpy` ya que la mayoría de los frameworks de machine learning se basan en conceptos muy similares.\n","\n","Entonces, comencemos con la importación de PyTorch. El paquete se llama `torch`, basado en su framework original [Torch](http://torch.ch/). Como primer paso, podemos comprobar su versión: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fR-MFlPfz2cw","outputId":"989e23b8-cb49-4a2a-bd56-f058b794b031","executionInfo":{"status":"ok","timestamp":1659558104762,"user_tz":300,"elapsed":1545,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["import torch\n","print(\"Usando torch\", torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Usando torch 1.12.0+cu113\n"]}]},{"cell_type":"markdown","metadata":{"id":"DUe5EPYH0fZB"},"source":["Al momento de escribir este tutorial, la versión estable actual es 1.12.0. Por lo tanto, debería ver la salida `Usando torch 1.12.0+cu113`. Si ve un número de versión más bajo, asegúrese de haber instalado el entorno correcto. En caso de que una versión posterior de PyTorch se publique durante el tiempo del curso, no se preocupe. La interfaz entre las versiones de PyTorch no cambia demasiado y, por lo tanto, todo el código también debería poder ejecutarse con versiones más nuevas.\n","\n","Como en todo framework de machine learning, PyTorch proporciona funciones estocásticas como generar números aleatorios. Sin embargo, una muy buena práctica es configurar su código para que sea reproducible con exactamente los mismos números aleatorios. Es por eso que establecemos una semilla a continuación."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WoyoHGB30WEZ","outputId":"5b32ce60-46de-478e-ac2b-b1db56188f20","executionInfo":{"status":"ok","timestamp":1659558104763,"user_tz":300,"elapsed":146,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["torch.manual_seed(42) # Para reproducibilidad"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f31db207af0>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"q2WyWS4k1JL4"},"source":["## Tensores\n","\n","![alt](https://i.pinimg.com/originals/80/c0/b1/80c0b157636aa8cb4b4e56180b8ab8e7.png)\n","\n","Los tensores son el equivalente de PyTorch a las matrices Numpy, con la adición de que también tienen soporte para la aceleración de GPU (hablaremos de esto más adelante). El nombre \"tensor\" es una generalización de conceptos que ya conoce. Por ejemplo, un vector es un tensor 1-D y una matriz un tensor 2-D. Cuando trabajemos con redes neuronales, usaremos tensores de varias formas y número de dimensiones.\n","\n","Las funciones más comunes que conoce de `numpy` también se pueden usar en tensores. En realidad, dado que los arreglos `numpy` son tan similares a los tensores, podemos convertir la mayoría de los tensores en arreglos `numpy` (y viceversa) pero no lo necesitamos con demasiada frecuencia."]},{"cell_type":"markdown","metadata":{"id":"9WulFolYDHH8"},"source":["### Inicialización\n","\n","Primero comencemos mirando diferentes formas de crear un tensor. Hay muchas opciones posibles, la más simple es llamar a `torch.empty` pasando la forma deseada como argumento de entrada. Esto creará un tensor no inicializado: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWnwHVEk1E7w","outputId":"a371a4ae-5078-4cee-d4de-1f8845ee44c5","executionInfo":{"status":"ok","timestamp":1659558104763,"user_tz":300,"elapsed":140,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# torch.empty(size): no inicializado\n","# scalar\n","x = torch.empty(1)\n","print('Scalar')\n","print(x)\n","\n","# vector, 1D\n","x = torch.empty(3)\n","print()\n","print('Vector 1D')\n","print(x)\n","\n","# matriz, 2D\n","x = torch.empty(2,3)\n","print()\n","print('Vector 2D')\n","print(x)\n","\n","# tensor, 3 dimensiones\n","x = torch.empty(2,2,3) \n","print()\n","print('Vector 3D')\n","print(x)\n","\n","# tensor, 4 dimensiones\n","x = torch.empty(2,2,2,3)\n","print()\n","print('Vector 4D')\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scalar\n","tensor([1.2620e-35])\n","\n","Vector 1D\n","tensor([1.2620e-35, 0.0000e+00, 5.0447e-44])\n","\n","Vector 2D\n","tensor([[1.2620e-35, 0.0000e+00, 5.0447e-44],\n","        [0.0000e+00,        nan, 0.0000e+00]])\n","\n","Vector 3D\n","tensor([[[1.2620e-35, 0.0000e+00, 5.0447e-44],\n","         [0.0000e+00,        nan, 0.0000e+00]],\n","\n","        [[1.3788e-14, 3.6423e-06, 2.0699e-19],\n","         [3.3738e-12, 7.4086e+28, 6.9397e+22]]])\n","\n","Vector 4D\n","tensor([[[[1.1020e-35, 0.0000e+00, 0.0000e+00],\n","          [0.0000e+00, 1.4013e-45, 0.0000e+00]],\n","\n","         [[0.0000e+00, 0.0000e+00, 0.0000e+00],\n","          [0.0000e+00, 0.0000e+00, 0.0000e+00]]],\n","\n","\n","        [[[1.4013e-45, 0.0000e+00, 1.4013e-45],\n","          [0.0000e+00, 1.4013e-45, 0.0000e+00]],\n","\n","         [[1.4013e-45, 0.0000e+00, 1.4013e-45],\n","          [0.0000e+00, 1.4013e-45, 0.0000e+00]]]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZaIqNzw95Sc2"},"source":["Para asignar valores directamente al tensor durante la inicialización, existen muchas alternativas que incluyen:\n","\n","* `torch.zeros`: Crea un tensor lleno de ceros\n","* `torch.ones`: Crea un tensor lleno de unos\n","* `torch.rand`: Crea un tensor con valores aleatorios muestreados uniformemente entre 0 y 1\n","* `torch.randn`: Crea un tensor con valores aleatorios muestreados a partir de una distribución normal con media 0 y varianza 1\n","* `torch.arange`: Crea un tensor que contiene los valores $N, N + 1, N + 2, ..., M$\n","* `torch.tensor(lista de entrada)`: Crea un tensor a partir de los elementos de la lista que proporcionemos"]},{"cell_type":"markdown","metadata":{"id":"AM-o1FY97ols"},"source":["**Directamente de datos**\n","\n","Los tensores se pueden crear directamente a partir de listas datos. El tipo de datos se infiere automáticamente. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqNZJOVs1vW-","outputId":"2aa38e6f-2c49-4564-acdd-a5aca44ff8eb","executionInfo":{"status":"ok","timestamp":1659558104764,"user_tz":300,"elapsed":136,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Crear de tensor a partir de una lista o matriz\n","data = [[1, 2], [3, 4]]\n","x = torch.tensor(data)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2],\n","        [3, 4]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"mA7QZcLa8Cha"},"source":["**Con valores aleatorios o constantes:**\n","\n","Podemos crear tensores con valores aleatorios siguiendo una distribución uniforme (``torch.rand``) o normal (``torch.randn``)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eyJxBKxX8MWw","outputId":"64db4de7-2ea4-4915-cc13-90da2e1ad484","executionInfo":{"status":"ok","timestamp":1659558104764,"user_tz":300,"elapsed":132,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# torch.rand(size): números aleatorios [0, 1]\n","x = torch.rand(5, 3)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.8823, 0.9150, 0.3829],\n","        [0.9593, 0.3904, 0.6009],\n","        [0.2566, 0.7936, 0.9408],\n","        [0.1332, 0.9346, 0.5936],\n","        [0.8694, 0.5677, 0.7411]])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNc9Ybhq9MeY","outputId":"609ed060-236d-4a69-9ebc-508295023353","executionInfo":{"status":"ok","timestamp":1659558104765,"user_tz":300,"elapsed":129,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# torch.randn(size): números aleatorios muestreados a partir de una distribución normal\n","x = torch.randn(2, 3, 4)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 0.5201,  1.6423, -0.1596, -0.4974],\n","         [ 0.4396, -0.7581,  1.0783,  0.8008],\n","         [-0.7746,  0.0349,  0.3211,  1.5736]],\n","\n","        [[-0.8455,  1.3123,  0.6872, -1.0892],\n","         [-0.4879, -1.4181,  0.8963,  0.0499],\n","         [ 2.2667,  1.1790, -0.4345, -1.3864]]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"WKNJSamK9gJ0"},"source":["Podemos obtener la forma de un tensor de la misma forma que en ``numpy`` (`x.shape`), o usando el método `.size`:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDXX6yOM9n_D","outputId":"b98cc270-ab99-4c48-a130-d71c97b8b162","executionInfo":{"status":"ok","timestamp":1659558104766,"user_tz":300,"elapsed":125,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["shape = x.shape\n","print(\"Shape:\", x.shape)\n","\n","size = x.size()\n","print(\"Size:\", size)\n","\n","dim1, dim2, dim3 = x.size()\n","print(\"Size:\", dim1, dim2, dim3)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape: torch.Size([2, 3, 4])\n","Size: torch.Size([2, 3, 4])\n","Size: 2 3 4\n"]}]},{"cell_type":"markdown","metadata":{"id":"uweQSLdE956T"},"source":["También podemos verificar el tipo de datos y el dispositivo en el que se almacenan:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_WzOhys-JPH","outputId":"9a68b876-d6da-4b3f-8069-10ae501425bc","executionInfo":{"status":"ok","timestamp":1659558104767,"user_tz":300,"elapsed":123,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.rand(3,4)\n","\n","print(\"Tamaño del tensor:\", x.shape)\n","print(\"Tipo de dato del tensor:\", x.dtype)\n","print(\"Dispositivo en el que está almacenado el tensor:\", x.device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del tensor: torch.Size([3, 4])\n","Tipo de dato del tensor: torch.float32\n","Dispositivo en el que está almacenado el tensor: cpu\n"]}]},{"cell_type":"markdown","metadata":{"id":"s0t9xq2T_MHw"},"source":["**Tensores de ceros y unos**\n","\n","Podemos crear tensores de ceros y unos de forma similar a los que se hace en numpy:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zp1G8WLq_V6w","outputId":"7fec6a96-f7d9-489b-f407-e0e129daa8df","executionInfo":{"status":"ok","timestamp":1659558104770,"user_tz":300,"elapsed":122,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# creamos tensores de unos y ceros acorde al tamaño específicado\n","shape = (3,4)\n","\n","# tensor de unos\n","x_ones = torch.ones(shape)\n","print('Tensor de unos')\n","print(x_ones)\n","\n","# tensor de ceros\n","print('\\nTensor de ceros')\n","x_zeros = torch.zeros(shape)\n","\n","print(x_zeros)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor de unos\n","tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]])\n","\n","Tensor de ceros\n","tensor([[0., 0., 0., 0.],\n","        [0., 0., 0., 0.],\n","        [0., 0., 0., 0.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"QEAFCslf_uwI"},"source":["También es posible especificar el tipo de dato al momento de crear el tensor:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvjA8ybf_vNG","outputId":"b3381bdf-0a08-4204-8fcb-2db17682340f","executionInfo":{"status":"ok","timestamp":1659558104771,"user_tz":300,"elapsed":119,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# especificamos tipos, por defecto se tiene el tipo de dato -> float32\n","x = torch.ones(5, 3, dtype=torch.float16)\n","print(x)\n","\n","# verificamos tipo\n","print(x.dtype)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]], dtype=torch.float16)\n","torch.float16\n"]}]},{"cell_type":"markdown","metadata":{"id":"oQaRKez4VNpm"},"source":["**A partir de otro tensor:**\n","\n","El nuevo tensor conserva las propiedades (forma, tipo de datos) del tensor del argumento, a menos que se anule explícitamente:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1O5ExRwMVSmx","outputId":"0cb1d5c2-9390-41a7-881d-dc4f26f99335","executionInfo":{"status":"ok","timestamp":1659558104772,"user_tz":300,"elapsed":115,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x_data = torch.tensor([[2,4], [6,7]])\n","print(\"Data original\", x_data)\n","\n","x = torch.ones_like(x_data) # retiene las propiedades de x_data\n","print(\"\\nTensor de unos\", x)\n","\n","x = torch.rand_like(x_data, dtype=torch.float) # sobreescribimos el tipo de dato de x_data\n","print(\"\\nTensor aleatorio:\", x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data original tensor([[2, 4],\n","        [6, 7]])\n","\n","Tensor de unos tensor([[1, 1],\n","        [1, 1]])\n","\n","Tensor aleatorio: tensor([[0.2036, 0.2018],\n","        [0.2018, 0.9497]])\n"]}]},{"cell_type":"markdown","source":["**Copiar tensor**\n","\n","Para copiar un tensor podemos hacer uso de [tensor.clone()](https://pytorch.org/docs/stable/generated/torch.clone.html), no es posible usar el operador de asignación directamente ya que cualquier cambio puede afectar al tensor original. Veamos un ejemplo:\n"],"metadata":{"id":"RcqcrZtamJ0t"}},{"cell_type":"code","source":["x_data = torch.tensor([[2,4], [6,7]])\n","print(\"Data original\", x_data)\n","\n","# Tratamos de copiar usando operador de asignación\n","copy = x_data\n","\n","# Modificamos un valor del tensor copy\n","copy[0][0] = 20\n","print(\"\\nTensor copia:\")\n","print(copy)\n","print(\"Tensor original:\")\n","print(x_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WuvA2M0uqd3L","executionInfo":{"status":"ok","timestamp":1659558104773,"user_tz":300,"elapsed":113,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"56317b67-0203-4fe7-c83d-9cc30b4620c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data original tensor([[2, 4],\n","        [6, 7]])\n","\n","Tensor copia:\n","tensor([[20,  4],\n","        [ 6,  7]])\n","Tensor original:\n","tensor([[20,  4],\n","        [ 6,  7]])\n"]}]},{"cell_type":"markdown","source":["Como podemos observar el cambio realizado en el tensor copy también se ve reflejado en el tensor original. Para evitar esto, al momento de hacer una copia es preferible usar [tensor.clone()](https://pytorch.org/docs/stable/generated/torch.clone.html)."],"metadata":{"id":"QCt0xx2Ar48W"}},{"cell_type":"code","source":["x_data = torch.tensor([[2,4], [6,7]])\n","print(\"Data original\", x_data)\n","\n","# Copiamos tensor usando .clone()\n","copy = x_data.clone()\n","\n","# Modificamos un valor del tensor copy\n","copy[0][0] = 20\n","\n","print(\"\\nTensor copia:\")\n","print(copy)\n","print(\"Tensor original:\")\n","print(x_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KbIELp32sCOq","executionInfo":{"status":"ok","timestamp":1659558104773,"user_tz":300,"elapsed":109,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"112c7c2f-ae4d-4fb4-86db-3bffe706a6f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data original tensor([[2, 4],\n","        [6, 7]])\n","\n","Tensor copia:\n","tensor([[20,  4],\n","        [ 6,  7]])\n","Tensor original:\n","tensor([[2, 4],\n","        [6, 7]])\n"]}]},{"cell_type":"markdown","source":["Podemos ver que ahora el cambio no afecta al tensor original."],"metadata":{"id":"_rL39gcusm_S"}},{"cell_type":"markdown","metadata":{"id":"NPMjdfw3-0kB"},"source":["### Tensor a Numpy y Numpy a Tensor\n","\n","Los tensores se pueden convertir en matrices numpy y de matrices numpy de nuevo en tensores. Para transformar una matriz numpy en un tensor, podemos usar la función `torch.from_numpy`:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wzNL0JUZ_AQ6","outputId":"74c8a418-a138-4ba3-cc3d-d0b787268ba9","executionInfo":{"status":"ok","timestamp":1659558104774,"user_tz":300,"elapsed":106,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x_np = np.array([[1, 2], [3, 4]])\n","x = torch.from_numpy(x_np)\n","\n","print(\"Arreglo en Numpy:\", x_np)\n","print(\"Tensor en PyTorch:\", x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Arreglo en Numpy: [[1 2]\n"," [3 4]]\n","Tensor en PyTorch: tensor([[1, 2],\n","        [3, 4]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"rBrl9IINYDib"},"source":["Los cambios en la matriz NumPy se reflejan en el tensor (operaciones in-place):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcyIudKAYJxF","outputId":"ee28b059-90d1-499f-bbe4-105742dbcfe6","executionInfo":{"status":"ok","timestamp":1659558104774,"user_tz":300,"elapsed":101,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x_np += 1\n","print(\"Arreglo en Numpy:\", x_np)\n","print(\"Tensor en PyTorch:\", x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Arreglo en Numpy: [[2 3]\n"," [4 5]]\n","Tensor en PyTorch: tensor([[2, 3],\n","        [4, 5]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"2xOMkUKNAmu4"},"source":["Para transformar un tensor de PyTorch en una matriz numpy, podemos usar la función `.numpy()` en tensores:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szbw7eelAoq3","outputId":"64b0c65e-5e66-4592-8001-7ef7419cacec","executionInfo":{"status":"ok","timestamp":1659558104775,"user_tz":300,"elapsed":98,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.arange(4)\n","x_np = x.numpy()\n","\n","print(\"Tensor en PyTorch:\", x)\n","print(\"Arreglo en Numpy:\", x_np)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor en PyTorch: tensor([0, 1, 2, 3])\n","Arreglo en Numpy: [0 1 2 3]\n"]}]},{"cell_type":"markdown","metadata":{"id":"vs27v2ywZIJc"},"source":["Un cambio en el tensor se refleja en la matriz NumPy (operaciones in-place):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M42Zzr_8ZUCt","outputId":"77a14c80-802e-49a9-f5b3-43bd4de4225b","executionInfo":{"status":"ok","timestamp":1659558104776,"user_tz":300,"elapsed":96,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x_np += 1\n","print(\"Tensor en PyTorch:\", x)\n","print(\"Arreglo en Numpy:\", x_np)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor en PyTorch: tensor([1, 2, 3, 4])\n","Arreglo en Numpy: [1 2 3 4]\n"]}]},{"cell_type":"markdown","metadata":{"id":"bHFYVLAtBBez"},"source":["La conversión de tensores a numpy requiere que el tensor esté en la CPU y no en la GPU. En caso de que tengamos un tensor en la GPU, debemos llamar a `.cpu()` en el tensor de antemano. Por lo tanto, obtenemos una línea como `x_np = tensor.cpu().numpy()`. "]},{"cell_type":"markdown","metadata":{"id":"Mt9L5QBCCIje"},"source":["### Operaciones de tensores\n","\n","La mayoría de las operaciones que existen en numpy, también existen en PyTorch. Se puede encontrar una lista completa de operaciones en la [documentación de PyTorch](https://pytorch.org/docs/stable/tensors.html#), pero revisaremos las más importantes aquí."]},{"cell_type":"markdown","metadata":{"id":"_EggC9VuJ-wN"},"source":["#### Operaciones elementwise\n","\n","La operación más sencilla es sumar dos tensores: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qCmNfV9iCOB5","outputId":"6fa2d812-2ea4-4442-f9b0-a18cbb24fd2a","executionInfo":{"status":"ok","timestamp":1659558104776,"user_tz":300,"elapsed":91,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x1 = torch.rand(2, 3)\n","x2 = torch.rand(2, 3)\n","y = x1 + x2\n","\n","print(\"X1\", x1)\n","print(\"X2\", x2)\n","print(\"\\nSuma elementwise\")\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X1 tensor([[0.6666, 0.9811, 0.0874],\n","        [0.0041, 0.1088, 0.1637]])\n","X2 tensor([[0.7025, 0.6790, 0.9155],\n","        [0.2418, 0.1591, 0.7653]])\n","\n","Suma elementwise\n","tensor([[1.3691, 1.6602, 1.0028],\n","        [0.2458, 0.2680, 0.9289]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"vqCdoCNfCdpA"},"source":["Llamar a `x1 + x2` crea un nuevo tensor que contiene la suma de las dos entradas. Sin embargo, también podemos usar operaciones in-place que se aplican directamente en la memoria de un tensor. Por lo tanto, cambiamos los valores de `x2` sin la posibilidad de volver a acceder a los valores de` x2` antes de la operación. A continuación se muestra un ejemplo: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9nU7lGyCnow","outputId":"f1e08d12-0cd4-4333-a517-c5a18b64c534","executionInfo":{"status":"ok","timestamp":1659558104777,"user_tz":300,"elapsed":87,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x1 = torch.rand(2, 3)\n","x2 = torch.rand(2, 3)\n","print(\"X1 (antes)\", x1)\n","print(\"X2 (antes)\", x2)\n","\n","x2.add_(x1)\n","print(\"X1 (después)\", x1)\n","print(\"X2 (después)\", x2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X1 (antes) tensor([[0.2979, 0.8035, 0.3813],\n","        [0.7860, 0.1115, 0.2477]])\n","X2 (antes) tensor([[0.6524, 0.6057, 0.3725],\n","        [0.7980, 0.8399, 0.1374]])\n","X1 (después) tensor([[0.2979, 0.8035, 0.3813],\n","        [0.7860, 0.1115, 0.2477]])\n","X2 (después) tensor([[0.9503, 1.4092, 0.7539],\n","        [1.5841, 0.9514, 0.3851]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"dHe0mrVFEvF8"},"source":["Las operaciones in-place generalmente se marcan con un sufijo de subrayado (por ejemplo, \"add_\" en lugar de \"add\"). Además de la suma podemos realizar otros tipos de operaciones:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQKLgReEEvb9","outputId":"944e3489-25b3-4c2f-e825-795154b7e1d5","executionInfo":{"status":"ok","timestamp":1659558104777,"user_tz":300,"elapsed":82,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# resta\n","y = x1 - x2\n","y = torch.sub(x1, x2)\n","print('Resta elementwise')\n","print(y)\n","\n","# multiplicación\n","y = x1 * x2\n","y = torch.mul(x1, x2)\n","print('\\nMultiplicación elementwise')\n","print(y)\n","\n","# división\n","y = x1 / x2\n","y = torch.div(x1, x2)\n","print('\\nDivisión elementwise')\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Resta elementwise\n","tensor([[-0.6524, -0.6057, -0.3725],\n","        [-0.7980, -0.8399, -0.1374]])\n","\n","Multiplicación elementwise\n","tensor([[0.2831, 1.1322, 0.2875],\n","        [1.2451, 0.1061, 0.0954]])\n","\n","División elementwise\n","tensor([[0.3135, 0.5702, 0.5059],\n","        [0.4962, 0.1172, 0.6432]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"vIcg4pnpL-4_"},"source":["Torch también proporciona muchas funciones matemáticas estándar; estos están disponibles como funciones en el módulo `torch` y como métodos de instancia en tensores:\n","\n","Puede encontrar una lista completa de todas las funciones matemáticas disponibles en la [documentación](https://pytorch.org/docs/stable/torch.html#pointwise-ops); muchas funciones en el módulo `torch` tienen métodos de instancia correspondientes en [objetos tensores](https://pytorch.org/docs/stable/tensors.html)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBJWHp8ZMjw3","outputId":"53720983-6851-4b25-a733-d14e9eb00aa0","executionInfo":{"status":"ok","timestamp":1659558104778,"user_tz":300,"elapsed":79,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n","\n","print('Raíz cuadrada:')\n","print(torch.sqrt(x))\n","print(x.sqrt())\n","\n","print('\\nFunciones trigonométricas:')\n","print(torch.sin(x))\n","print(x.sin())\n","print(torch.cos(x))\n","print(x.cos())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Raíz cuadrada:\n","tensor([[1.0000, 1.4142, 1.7321, 2.0000]])\n","tensor([[1.0000, 1.4142, 1.7321, 2.0000]])\n","\n","Funciones trigonométricas:\n","tensor([[ 0.8415,  0.9093,  0.1411, -0.7568]])\n","tensor([[ 0.8415,  0.9093,  0.1411, -0.7568]])\n","tensor([[ 0.5403, -0.4161, -0.9900, -0.6536]])\n","tensor([[ 0.5403, -0.4161, -0.9900, -0.6536]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"TbHmoKbJM_YT"},"source":["#### Operaciones de reducción\n","\n","Hasta ahora hemos visto operaciones aritméticas básicas en tensores que operan elemento a elemento. A veces, es posible que deseemos realizar operaciones que agreguen una parte o la totalidad de un tensor, como una suma; estas se denominan operaciones de **reducción**.\n","\n","Al igual que las operaciones de elementos anteriores, la mayoría de las operaciones de reducción están disponibles como funciones en el módulo `torch` y como métodos de instancia en objetos` tensor`.\n","\n","La operación de reducción más simple es la suma. Podemos usar la función [`.sum()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sum) \\(o de manera equivalente [`torch.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html)) para reducir un tensor completo o para reducir solo una dimensión del tensor usando el argumento `dim`:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtJsOBmUO5p_","outputId":"27f82bc8-8ff5-4001-dfcd-972286425235","executionInfo":{"status":"ok","timestamp":1659558104779,"user_tz":300,"elapsed":72,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.tensor([[1, 2, 3], \n","                  [4, 5, 6]], dtype=torch.float32)\n","print('Tensor original:')\n","print(x)\n","\n","print('\\nSuma sobre todo el tensor:')\n","print(torch.sum(x))\n","print(x.sum())\n","\n","# Suma de todas las filas (i.e. por cada columna)\n","print('\\nSuma de todas las filas:')\n","print(torch.sum(x, dim=0))\n","print(x.sum(dim=0))\n","\n","# Suma de todas las columnas (i.e. por cada fila)\n","print('\\nSuma de columnas:')\n","print(torch.sum(x, dim=1))\n","print(x.sum(dim=1))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original:\n","tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n","\n","Suma sobre todo el tensor:\n","tensor(21.)\n","tensor(21.)\n","\n","Suma por filas:\n","tensor([5., 7., 9.])\n","tensor([5., 7., 9.])\n","\n","Suma por columnas:\n","tensor([ 6., 15.])\n","tensor([ 6., 15.])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Q_y31nm4PjUR"},"source":["Otras operaciones de reducción útiles incluyen [`mean`](https://pytorch.org/docs/stable/torch.html#torch.mean), [`min`](https://pytorch.org/docs/stable/torch.html#torch.min) y [`max`](https://pytorch.org/docs/stable/torch.html#torch.max). Puede encontrar una lista completa de todas las operaciones de reducción disponibles en la [documentación](https://pytorch.org/docs/stable/torch.html#reduction-ops).\n","\n","Algunas operaciones de reducción devuelven más de un valor; por ejemplo, `min` devuelve tanto el valor mínimo sobre la dimensión especificada como el índice donde se produce el valor mínimo:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbFoeQTtPzl8","outputId":"16a4ee3d-a969-4a84-f1ee-fb8ad17e164a","executionInfo":{"status":"ok","timestamp":1659558104780,"user_tz":300,"elapsed":69,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.tensor([[2, 4, 3, 5], [3, 3, 5, 2]], dtype=torch.float32)\n","print('Tensor original:')\n","print(x, x.shape)\n","\n","# Mínimo general solo devuelve un valor único\n","print('\\nMínimo general: ', x.min())\n","\n","# Calcular el mínimo a lo largo de cada columna; obtenemos tanto el valor como la ubicación:\n","# El mínimo de la primera columna es 2 y aparece en el índice 0;\n","# el mínimo de la segunda columna es 3 y aparece en el índice 1; etc\n","col_min_vals, col_min_idxs = x.min(dim=0)\n","print('\\nMínimo a lo largo de cada columna:')\n","print('valores:', col_min_vals)\n","print('indices:', col_min_idxs)\n","\n","# Calcule el mínimo a lo largo de cada fila; obtenemos tanto el valor como el mínimo\n","row_min_vals, row_min_idxs = x.min(dim=1)\n","print('\\nMínimo a lo largo de cada fila:')\n","print('valores:', row_min_vals)\n","print('indices:', row_min_idxs)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original:\n","tensor([[2., 4., 3., 5.],\n","        [3., 3., 5., 2.]]) torch.Size([2, 4])\n","\n","Mínimo general:  tensor(2.)\n","\n","Mínimo a lo largo de cada columna:\n","valores: tensor([2., 3., 3., 2.])\n","indices: tensor([0, 1, 0, 1])\n","\n","Mínimo a lo largo de cada fila:\n","valores: tensor([2., 2.])\n","indices: tensor([0, 3])\n"]}]},{"cell_type":"markdown","metadata":{"id":"5nbJjxUZRMH_"},"source":["Operaciones de reducción *reducen* las dimensiones de tensores: la dimensión sobre la que se realiza la reducción se eliminará de la forma de la salida. Si pasa `keepdim=True` a una operación de reducción, la dimensión especificada no se eliminará; en cambio, el tensor de salida tendrá una forma de 1 en esa dimensión.\n","\n","Cuando trabaja con tensores multidimensionales, pensar en filas y columnas puede resultar confuso; en cambio, es más útil pensar en la forma que resultará de cada operación. Por ejemplo:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dm5Z6My7RlwT","outputId":"3f5764fc-f97a-42f1-fc6f-ef5cd1bad8c1","executionInfo":{"status":"ok","timestamp":1659558104780,"user_tz":300,"elapsed":67,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Creamos tensor de tamaño (128, 10, 3, 64, 64)\n","x = torch.randn(128, 10, 3, 64, 64)\n","print(x.shape)\n","\n","# Media sobre la dimensión 1; tamaño ahora es (128, 3, 64, 64)\n","x = x.mean(dim=1)\n","print(x.shape)\n","\n","# Suma sobre la dimensión 2; tamaño ahora es (128, 3, 64)\n","x = x.sum(dim=2)\n","print(x.shape)\n","\n","# Tomamos la media sobre la dimensión 1, pero evitamos que se elimine la dimensión\n","# pasando keepdim=True; tamaño ahora es (128, 1, 64)\n","x = x.mean(dim=1, keepdim=True)\n","print(x.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([128, 10, 3, 64, 64])\n","torch.Size([128, 3, 64, 64])\n","torch.Size([128, 3, 64])\n","torch.Size([128, 1, 64])\n"]}]},{"cell_type":"markdown","metadata":{"id":"7FXAF2u4FiKb"},"source":["### Operaciones matriciales\n","\n","Otras operaciones de uso común incluyen multiplicaciones de matrices, que son esenciales para las redes neuronales. Muy a menudo, tenemos un vector de entrada $\\mathbf{x}$, que se transforma usando una matriz de peso aprendida $\\mathbf{W}$. Hay varias formas y funciones para realizar la multiplicación de matrices, algunas de las cuales se enumeran a continuación:\n","\n","* `torch.matmul`: Realiza el producto matricial sobre dos tensores, donde el comportamiento específico depende de las dimensiones. Si ambas entradas son matrices (tensores bidimensionales), realiza el producto matricial estándar. Para entradas de mayor dimensión, la función admite broadcasting (para obtener más detalles, consulte la [documentación](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)). También se puede escribir como `a @ b`, similar a numpy.\n","* `torch.mm`: realiza el producto matricial sobre dos matrices, pero no admite broadcasting (consulte la [documentación](https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm))\n","* `torch.bmm`: Realiza el producto de matriz con una dimensión de lote de soporte (batch size). Si el primer tensor $T$ tiene la forma ($b \\times n \\times m$), y el segundo tensor $R$ ($b \\times m \\times p$), la salida $O$ tiene la forma ( $b \\times n \\times p $), y se ha calculado realizando multiplicaciones de matriz $b$ de las submatrices de $T$ y $R$: $O_i = T_i @ R_i$\n","* `torch.einsum`: Realiza multiplicaciones de matrices y sumas (es decir, sumas de productos) utilizando la convención de suma de Einstein. Este operador facilita varias operaciones matriciales. Una explicación de la suma de Einstein la puede encontrar en el [siguiente blog](https://theaisummer.com/einsum-attention/).\n","\n","Por lo general, usamos `torch.matmul` o` torch.bmm`. Podemos probar una multiplicación de matrices con `torch.matmul` a continuación. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yq03VqjtLBd7","outputId":"33bef825-a21e-4a7c-d598-d4777ebfedfd","executionInfo":{"status":"ok","timestamp":1659558104781,"user_tz":300,"elapsed":66,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.arange(6)\n","x = x.view(-1, 3)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2],\n","        [3, 4, 5]])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHw79aihLGAf","outputId":"6a48ad98-e6fa-4515-b4ab-2c4b8b5bb295","executionInfo":{"status":"ok","timestamp":1659558104782,"user_tz":300,"elapsed":65,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["W = torch.arange(9).view(3, 3) # Podemos empilar varias operaciones en una sola linea\n","print(W)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2],\n","        [3, 4, 5],\n","        [6, 7, 8]])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gPLcdTT3LQMQ","outputId":"150c6f72-6958-4a9e-bded-26e129d4fbde","executionInfo":{"status":"ok","timestamp":1659558104783,"user_tz":300,"elapsed":64,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["h = torch.matmul(x, W) # Multiplicación de matrices\n","print(h)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[15, 18, 21],\n","        [42, 54, 66]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"MzA0LM80XU63"},"source":["**Tensores de un solo elemento**\n","\n","Si tiene un tensor de un elemento, por ejemplo, agregando todos\n","valores de un tensor en un valor, puede convertirlo en un valor numérico de Python usando ``item ()``: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdtYKiX4XbVR","outputId":"3be44a73-3c68-4fe7-a253-1abd5c0079d4","executionInfo":{"status":"ok","timestamp":1659558104784,"user_tz":300,"elapsed":62,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["sum = h.sum()\n","value = sum.item()\n","print(value, type(value))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["216 <class 'int'>\n"]}]},{"cell_type":"markdown","metadata":{"id":"SOQ6_TtHC7As"},"source":["### Operaciones de cambio de tamaño\n","\n","Otra operación común tiene como objetivo cambiar la forma de un tensor. Un tensor de tamaño (2,3) se puede reorganizar a cualquier otra forma con el mismo número de elementos (por ejemplo, un tensor de tamaño (6), o (3,2), ...). En PyTorch, esta operación se llama [`.view()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.view).\n","\n","Podemos usar `.view ()` para aplanar matrices en vectores, y para convertir vectores de 1 dimensión en matrices fila o columna de 2 dimensiones:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ym8WfYFBmm1g","outputId":"64485ab7-1c92-4404-91db-aa8fe4e08d6c","executionInfo":{"status":"ok","timestamp":1659558104786,"user_tz":300,"elapsed":62,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x0 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n","print('Tensor original')\n","print(x0)\n","print('tamaño:', x0.shape)\n","\n","# Aplanamos (Flatten) x0 en un vector de 1 dimensión (8,)\n","x1 = x0.view(8)\n","print('\\nTensor aplanado:')\n","print(x1)\n","print('tamaño:', x1.shape)\n","\n","# Convertimos x1 a un vector fila de dos dimensiones de tamaño (1,8)\n","x2 = x1.view(1, 8)\n","print('\\nVector fila:')\n","print(x2)\n","print('tamaño:', x2.shape)\n","\n","# Convertimos x1 a un vector columna de dos dimensiones de tamaño (8,1)\n","x3 = x1.view(8, 1)\n","print('\\nVector columna:')\n","print(x3)\n","print('tamaño:', x3.shape)\n","\n","# Convertimos x1 a un tensor de tres dimensiones de tamaño (2, 2, 2)\n","x4 = x1.view(2, 2, 2)\n","print('\\nTensor de 3 dimensiones:')\n","print(x4)\n","print('tamaño:', x4.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original\n","tensor([[1, 2, 3, 4],\n","        [5, 6, 7, 8]])\n","tamaño: torch.Size([2, 4])\n","\n","Tensor aplanado:\n","tensor([1, 2, 3, 4, 5, 6, 7, 8])\n","tamaño: torch.Size([8])\n","\n","Vector fila:\n","tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n","tamaño: torch.Size([1, 8])\n","\n","Vector columna:\n","tensor([[1],\n","        [2],\n","        [3],\n","        [4],\n","        [5],\n","        [6],\n","        [7],\n","        [8]])\n","tamaño: torch.Size([8, 1])\n","\n","Tensor de 3 dimensiones:\n","tensor([[[1, 2],\n","         [3, 4]],\n","\n","        [[5, 6],\n","         [7, 8]]])\n","tamaño: torch.Size([2, 2, 2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"UrJoRQwCn0f6"},"source":["Las llamadas a `.view ()` pueden incluir un único argumento -1; esto pone elementos necesarios en esa dimensión para que la salida tenga la misma forma que la entrada. Esto hace que sea fácil escribir algunas operaciones de cambio de dimensión de una manera que sea independiente de la forma del tensor:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKG9qusZoNTU","outputId":"459c0286-324d-410d-e793-83fc0fcfbe5c","executionInfo":{"status":"ok","timestamp":1659558104787,"user_tz":300,"elapsed":61,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Podemos reutilizar estas funciones para tensores de diferentes formas\n","def flatten(x):\n","  return x.view(-1)\n","\n","def make_row_vec(x):\n","  return x.view(1, -1)\n","\n","x0 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","x0_flat = flatten(x0)\n","x0_row = make_row_vec(x0)\n","print('x0:')\n","print(x0)\n","print('x0_flat:')\n","print(x0_flat)\n","print('x0_row:')\n","print(x0_row)\n","\n","x1 = torch.tensor([[1, 2], [3, 4]])\n","x1_flat = flatten(x1)\n","x1_row = make_row_vec(x1)\n","print('\\nx1:')\n","print(x1)\n","print('x1_flat:')\n","print(x1_flat)\n","print('x1_row:')\n","print(x1_row)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x0:\n","tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","x0_flat:\n","tensor([1, 2, 3, 4, 5, 6])\n","x0_row:\n","tensor([[1, 2, 3, 4, 5, 6]])\n","\n","x1:\n","tensor([[1, 2],\n","        [3, 4]])\n","x1_flat:\n","tensor([1, 2, 3, 4])\n","x1_row:\n","tensor([[1, 2, 3, 4]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"IyhesWk2pzU4"},"source":["Otro ejemplo usando -1 en la primera dimensión:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BXRrxebop1SP","outputId":"df41a582-640c-47cb-9d04-4b59c99e381a","executionInfo":{"status":"ok","timestamp":1659558104788,"user_tz":300,"elapsed":57,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.arange(6)\n","print('x')\n","print(x)\n","\n","# si una de las dimensiones es -1, pytorch determinará automáticamente el tamaño necesario \n","x = x.view(-1, 2) # el tamaño -1 se infiere de otras dimensiones \n","print('\\nx con nueva dimensión')\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x\n","tensor([0, 1, 2, 3, 4, 5])\n","\n","x con nueva dimensión\n","tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"SBP75ix8vYW7"},"source":["#### Intercambio de ejes\n","\n","Otra operación común de cambio de dimensiones que quizás desee realizar es la transposición de una matriz. Puede que se sorprenda si intenta transponer una matriz con `.view()`: La función `view()` toma elementos en orden de fila, por lo que **no puede transponer matrices con `.view()`** .\n","\n","En general, solo debe usar `.view()` para agregar nuevas dimensiones a un tensor, o para colapsar dimensiones adyacentes de un tensor.\n","\n","Para otros tipos de operaciones de reshaping, generalmente necesita usar una función que pueda intercambiar ejes de un tensor. La función más simple de este tipo es `.t()`, específicamente para transposición de matrices. Está disponible como [función en el módulo `torch`](https://pytorch.org/docs/stable/generated/torch.t.html#torch.t) y como [método de instancia de tensor]( https://pytorch.org/docs/stable/tensors.html#torch.Tensor.t):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPW7Q9SxxFoX","outputId":"69037be8-2f3b-4bcb-9322-5ee34d59e801","executionInfo":{"status":"ok","timestamp":1659558104789,"user_tz":300,"elapsed":52,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","print('Matriz original:')\n","print(x)\n","print('\\nTranspuesta con view NO FUNCIONA!') #conversion sigue un orden contiguo\n","print(x.view(3, 2))\n","print('\\nMatriz transpuesta:') #convertimos filas en columnas\n","print(torch.t(x))\n","print(x.t())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Matriz original:\n","tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","\n","Transpuesta con view NO FUNCIONA!\n","tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n","\n","Matriz transpuesta:\n","tensor([[1, 4],\n","        [2, 5],\n","        [3, 6]])\n","tensor([[1, 4],\n","        [2, 5],\n","        [3, 6]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"CiZr_5VvxjvL"},"source":["Para tensores con más de dos dimensiones, podemos usar la función [`torch.transpose`](https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose) para intercambiar dimensiones arbitrarias, o el método [`.permute`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute) para permutar dimensiones arbitrariamente"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3z7MWeSxqxu","outputId":"aac72366-8d8d-4c7a-bf39-6d9e63348541","executionInfo":{"status":"ok","timestamp":1659558104790,"user_tz":300,"elapsed":48,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Creamos tensor de tamaño (2, 3, 4)\n","x0 = torch.tensor([\n","     [[1,  2,  3,  4],\n","      [5,  6,  7,  8],\n","      [9, 10, 11, 12]],\n","     [[13, 14, 15, 16],\n","      [17, 18, 19, 20],\n","      [21, 22, 23, 24]]])\n","print('Tensor original:')\n","print(x0)\n","print('tamaño:', x0.shape)\n","\n","# Intercambiar ejes 1 y 2; el tamaño es (2, 4, 3)\n","x1 = x0.transpose(1, 2)\n","print('\\nIntercambio de ejes 1 y 2:')\n","print(x1)\n","print(x1.shape)\n","\n","# Permutar ejes; el argumento (1, 2, 0) significa:\n","# - Hacer que la antigua dimensión 1 aparezca en la dimensión 0;\n","# - Hacer que la antigua dimensión 2 aparezca en la dimensión 1;\n","# - Hacer que la antigua dimensión 0 aparezca en la dimensión 2\n","# Esto da como resultado un tensor de forma (3, 4, 2)\n","x2 = x0.permute(1, 2, 0)\n","print('\\nEjes permutados')\n","print(x2)\n","print('tamaño:', x2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original:\n","tensor([[[ 1,  2,  3,  4],\n","         [ 5,  6,  7,  8],\n","         [ 9, 10, 11, 12]],\n","\n","        [[13, 14, 15, 16],\n","         [17, 18, 19, 20],\n","         [21, 22, 23, 24]]])\n","tamaño: torch.Size([2, 3, 4])\n","\n","Intercambio de ejes 1 y 2:\n","tensor([[[ 1,  5,  9],\n","         [ 2,  6, 10],\n","         [ 3,  7, 11],\n","         [ 4,  8, 12]],\n","\n","        [[13, 17, 21],\n","         [14, 18, 22],\n","         [15, 19, 23],\n","         [16, 20, 24]]])\n","torch.Size([2, 4, 3])\n","\n","Ejes permutados\n","tensor([[[ 1, 13],\n","         [ 2, 14],\n","         [ 3, 15],\n","         [ 4, 16]],\n","\n","        [[ 5, 17],\n","         [ 6, 18],\n","         [ 7, 19],\n","         [ 8, 20]],\n","\n","        [[ 9, 21],\n","         [10, 22],\n","         [11, 23],\n","         [12, 24]]])\n","tamaño: torch.Size([3, 4, 2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"setS8A-xy9do"},"source":["La diferencia entre `.transpose()`. y `.permute()`. es que el `.tranpose(dim0, dim1)`. acepta dos argumentos (dimensiones) mientras que `.permute(*dims)` es más general y acepta una tupla.  "]},{"cell_type":"markdown","metadata":{"id":"-QH80dArtLXc"},"source":["#### Tensores contiguos y reshape\n","\n","Para cambiar el tamaño de los tensores también podemos usar el método [`.reshape()`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape). Existe una sutil diferencia entre `reshape()` y `view()`: `view()` requiere que los datos se almacenen de forma contigua en la memoria. Puede consultar esta [respuesta](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch) de StackOverflow o esta publicación de [blog de Edward Yang](http://blog.ezyang.com/2019/05/pytorch-internals/) para obtener más información. En términos simples, contiguo significa que la forma en que nuestros datos se colocan en la memoria es la misma que la forma en que leeríamos los elementos de ella. Esto sucede porque algunos métodos, como `transpose()` y `view()`, en realidad no cambian cómo se almacenan nuestros datos en la memoria. Simplemente cambian la metainformación sobre nuestro tensor, de modo que cuando lo usemos veremos los elementos en el orden que esperamos.\n","\n","`reshape()` llama a `view()` internamente si los datos se almacenan de forma contigua, si no, devuelve una copia. La diferencia aquí no es demasiado importante para los tensores básicos, pero si realiza operaciones que hacen que el almacenamiento subyacente de los datos no sea contiguo (como tomar una transposición), tendrá problemas al usar `view()`. Si desea hacer coincidir la forma en que se almacena su tensor en la memoria con la forma en que se usa, puede usar el método [`.contiguous()`](https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZmnCmIe7tyWT","outputId":"259e98bf-f03c-49ea-fe95-4fe13ada57be","executionInfo":{"status":"ok","timestamp":1659558104791,"user_tz":300,"elapsed":48,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x0 = torch.randn(2, 3, 4)\n","\n","try:\n","  # Esta secuencia de operaciones fallará\n","  x1 = x0.transpose(1, 2).view(8, 3)\n","except RuntimeError as e:\n","  print(type(e), e)\n","  \n","# Podemos resolver el problema usando .contiguous () o .reshape()\n","x1 = x0.transpose(1, 2).contiguous().view(8, 3)\n","x2 = x0.transpose(1, 2).reshape(8, 3)\n","print('Tamaño de x1: ', x1.shape)\n","print('Tamaño de x2: ', x2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'RuntimeError'> view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n","Tamaño de x1:  torch.Size([8, 3])\n","Tamaño de x2:  torch.Size([8, 3])\n"]}]},{"cell_type":"markdown","source":["#### Squeeze y Unsqueeze\n","Squeeze y unsqueeze son métodos muy útiles para agregar y eliminar una dimensión del tensor."],"metadata":{"id":"ScLGT_59TkKI"}},{"cell_type":"markdown","source":["##### Squeeze\n","\n","Cuando realizamos `squeeze` de un tensor, se eliminan las dimensiones de tamaño 1. Los elementos del tensor original se reordenan con las dimensiones restantes. Por ejemplo, si el tensor de entrada tiene la forma: ($m \\times 1 \\times n \\times 1$), entonces el tensor de salida después de la operación de `squeeze` tendrá la forma: ($m \\times n$). Para realizar la operación `squeeze` en un tensor podemos aplicar el método [torch.squeeze()](https://pytorch.org/docs/stable/generated/torch.squeeze.html)."],"metadata":{"id":"ZtqTgOcQVxi-"}},{"cell_type":"code","source":["# Creamos tensor\n","x = torch.randn(3,1,2,1,4)\n","# Motramos el tamaño del tensor\n","print(\"Tamaño del tensor inicial:\\n\",x.size())\n"," \n","# Operación de squeeze\n","output = torch.squeeze(x)\n","# Mostramos el tensor luego de la operación\n","print(\"Tamaño después squeeze:\\n\",output.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIafkpEcZVek","executionInfo":{"status":"ok","timestamp":1659558105864,"user_tz":300,"elapsed":1119,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"c7a1af26-0522-4e78-b80b-aecbcd1980c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del tensor inicial:\n"," torch.Size([3, 1, 2, 1, 4])\n","Tamaño después squeeze:\n"," torch.Size([3, 2, 4])\n"]}]},{"cell_type":"markdown","source":["Podemos observar que en este ejemplo todas las dimensiones iguales a $1$ son eliminadas. Veamos otro ejemplo donde podemos elegir la dimensión a eliminar explícitamente:"],"metadata":{"id":"e16a7eFybWU0"}},{"cell_type":"code","source":["# Creamos tensor\n","x = torch.randn(3,1,2,1,4)\n","print(\"Dimensión del tensor inicial:\", x.dim())\n","print(\"Tamaño del tensor inicial:\\n\",x.size())\n"," \n","# Squeeze el tensor en la dimensión 0\n","output = torch.squeeze(x,dim=0)\n","print(\"Tamaño después squeeze en dim=0:\\n\",\n","      output.size())\n"," \n","# Squeeze el tensor en la dimensión 1\n","output = torch.squeeze(x,dim=1)\n","print(\"Tamaño después squeeze en dim=1:\\n\",\n","      output.size())\n"," \n","# Squeeze el tensor en la dimensión 2\n","output = torch.squeeze(x,dim=2)\n","print(\"Tamaño después squeeze en dim=2:\\n\",\n","      output.size())\n"," \n","# Squeeze el tensor en la dimensión 3\n","output = torch.squeeze(x,dim=3)\n","print(\"Tamaño después squeeze en dim=3:\\n\",\n","      output.size())\n"," \n","# Squeeze el tensor en la dimensión 4\n","output = torch.squeeze(x,dim=4)\n","print(\"Tamaño después squeeze en dim=4:\\n\",\n","      output.size())\n","# output = torch.squeeze(input,dim=5) # Error"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zF8W6011blDr","executionInfo":{"status":"ok","timestamp":1659558105865,"user_tz":300,"elapsed":102,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"d7fb4637-23ce-43ff-b5ba-e65b75b80a52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimensión del tensor inicial: 5\n","Tamaño del tensor inicial:\n"," torch.Size([3, 1, 2, 1, 4])\n","Tamaño después squeeze en dim=0:\n"," torch.Size([3, 1, 2, 1, 4])\n","Tamaño después squeeze en dim=1:\n"," torch.Size([3, 2, 1, 4])\n","Tamaño después squeeze en dim=2:\n"," torch.Size([3, 1, 2, 1, 4])\n","Tamaño después squeeze en dim=3:\n"," torch.Size([3, 1, 2, 4])\n","Tamaño después squeeze en dim=4:\n"," torch.Size([3, 1, 2, 1, 4])\n"]}]},{"cell_type":"markdown","source":["Notar que cuando hacemos `squeeze` del tensor en la dimensión 0, no hay cambio en la forma del tensor de salida. Cuando hacemos `squeeze` en la dimensión 1 o en la dimensión 3 (ambas son de tamaño 1), solo esta dimensión se elimina en el tensor de salida. Cuando hacemos `squeeze` en la dimensión 2 o la dimensión 4, no hay cambio en la forma del tensor de salida."],"metadata":{"id":"5IlzEppqea5j"}},{"cell_type":"markdown","source":["##### Unsqueeze\n","\n","Cuando realizamos `unsqueeze` un tensor, se inserta una nueva dimensión de tamaño 1 en la posición especificada. Siempre una operación de `unsqueeze` aumenta la dimensión del tensor de salida. Por ejemplo, si el tensor de entrada tiene la forma: ($m \\times n$) y queremos insertar una nueva dimensión en la posición 1, entonces el tensor de salida después de descomprimir tendrá la forma: ($m \\times 1 \\times n$). Para realizar la operación `unsqueeze` en un tensor podemos aplicar el método [torch.unsqueeze()](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)."],"metadata":{"id":"nBVT771VfSuH"}},{"cell_type":"code","source":["# Creamos tensor\n","x = torch.randn(3,2,4)\n","# Motramos la dimensión y el tamaño del tensor\n","print(\"Dimensión del tensor inicial:\", x.dim())\n","print(\"Tamaño del tensor inicial:\\n\",x.size())\n"," \n","# Unsqueeze el tensor en la dimensión 0\n","output = torch.unsqueeze(x,dim=0)\n","print(\"Tamaño después squeeze en dim=0:\\n\",\n","      output.size())\n","\n","# Unsqueeze el tensor en la dimensión 1\n","output = torch.unsqueeze(x,dim=1)\n","print(\"Tamaño después squeeze en dim=1:\\n\",\n","      output.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tIu5fAh_ghsl","executionInfo":{"status":"ok","timestamp":1659558105866,"user_tz":300,"elapsed":100,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"bf88086b-7362-48fa-8558-f15f1edbf664"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimensión del tensor inicial: 3\n","Tamaño del tensor inicial:\n"," torch.Size([3, 2, 4])\n","Tamaño después squeeze en dim=0:\n"," torch.Size([1, 3, 2, 4])\n","Tamaño después squeeze en dim=1:\n"," torch.Size([3, 1, 2, 4])\n"]}]},{"cell_type":"markdown","source":["### Combinando tensores\n","\n","Podemos combinar tensor usando las operaciones de concatenación (`torch.cat`) y empilado (`torch.stack`)"],"metadata":{"id":"0yB_lIlpv8rG"}},{"cell_type":"markdown","source":["#### Concatenación\n","\n","Para concatenar una secuencia de tensores podemos usar la función [torch.cat()](https://pytorch.org/docs/stable/generated/torch.cat.html) que permite concatenar a lo largo de la misma dimensión. Los tensores deben tener la misma forma (excepto en la dimensión que desea realizar la concatenación) o estar vacíos."],"metadata":{"id":"3fIw6oXo11XK"}},{"cell_type":"code","source":["# Inicializamos tensores\n","x = torch.tensor([2, 3, 4, 5])\n","y = torch.tensor([4, 10, 30])\n","z = torch.tensor([7, 22, 4, 8, 3, 6])\n","\n","# Mostramos sus tamaños\n","print(\"Tamaño de tensor x: \", x.size())\n","print(\"Tamaño de tensor y: \", y.size())\n","print(\"Tamaño de tensor z: \", z.size()) \n","\n","# Concatenamos a lo largo de la dimensión 0\n","xyz = torch.cat( (x,y,z) , dim=0)\n","\n","print(\"\\nTensor concatenado: \")\n","print(xyz)\n","print(\"Tamaño de tensor xyz: \", xyz.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y-ngch_D31v8","executionInfo":{"status":"ok","timestamp":1659558105867,"user_tz":300,"elapsed":95,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"db44bfa5-99c8-424d-86b8-076596256282"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño de tensor x:  torch.Size([4])\n","Tamaño de tensor y:  torch.Size([3])\n","Tamaño de tensor z:  torch.Size([6])\n","\n","Tensor concatenado: \n","tensor([ 2,  3,  4,  5,  4, 10, 30,  7, 22,  4,  8,  3,  6])\n","Tamaño de tensor xyz:  torch.Size([13])\n"]}]},{"cell_type":"markdown","source":["Podemos concatenar tensores de mayor dimensionalidad:"],"metadata":{"id":"OkNPXaxo6CQV"}},{"cell_type":"code","source":["# Inicializamos tensores\n","x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","y = torch.tensor([[10, 20, 30]])\n","\n","# Mostramos sus tamaños\n","print(\"Tamaño de tensor x: \", x.size())\n","print(\"Tamaño de tensor y: \", y.size())\n","\n","# Concatenamos a lo largo de la dimensión 0\n","xy = torch.cat( (x,y) , dim=0)\n","\n","print(\"\\nTensor concatenado: \")\n","print(xy)\n","print(\"Tamaño de tensor xy: \", xy.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBGmkut66GBL","executionInfo":{"status":"ok","timestamp":1659558105867,"user_tz":300,"elapsed":86,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"bfafda04-369d-43f8-c24b-72e858115782"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño de tensor x:  torch.Size([2, 3])\n","Tamaño de tensor y:  torch.Size([1, 3])\n","\n","Tensor concatenado: \n","tensor([[ 1,  2,  3],\n","        [ 4,  5,  6],\n","        [10, 20, 30]])\n","Tamaño de tensor xy:  torch.Size([3, 3])\n"]}]},{"cell_type":"markdown","source":["En el ejemplo anterior concatenamos en la dimensión 0, es decir, adicionamos una fila al tensor original. Notar que la dimensión en la que se hace la concatenación no necesariamente es la misma, las otras dimensiones si deben ser iguales.\n","\n","Ahora realizaremos la concatenación por columnas (dimensión 1)"],"metadata":{"id":"WC4hXM956ubz"}},{"cell_type":"code","source":["# Inicializamos tensores\n","x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","z = torch.tensor([[10],[20]])\n","\n","# Mostramos sus tamaños\n","print(\"Tamaño de tensor x: \", x.size())\n","print(\"Tamaño de tensor z: \", y.size())\n","\n","# Concatenmos a lo largo de la dimensión 1\n","xz = torch.cat( (x,z) , dim=1)\n","\n","print(\"\\nTensor concatenado: \")\n","print(xz)\n","print(\"Tamaño de tensor xz: \", xz.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xvOBZLgW7BD6","executionInfo":{"status":"ok","timestamp":1659558105868,"user_tz":300,"elapsed":85,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"7314a945-424a-4386-e0a0-8164a70d0d44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño de tensor x:  torch.Size([2, 3])\n","Tamaño de tensor z:  torch.Size([1, 3])\n","\n","Tensor concatenado: \n","tensor([[ 1,  2,  3, 10],\n","        [ 4,  5,  6, 20]])\n","Tamaño de tensor xz:  torch.Size([2, 4])\n"]}]},{"cell_type":"markdown","source":["#### Empilado\n","\n","La operación de empilado también une una secuencia de tensores pero sobre una nueva dimensión. Además, aquí los tensores deben ser del mismo tamaño. Para empilar una secuencia de tensores podemos usar la función [torch.stack()](https://pytorch.org/docs/stable/generated/torch.stack.html)"],"metadata":{"id":"CGHQp9re8dsA"}},{"cell_type":"code","source":["# Inicializamos tensores\n","x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n","y = torch.tensor([[10, 20, 30, 40], [50, 60, 70, 80], [90, 100, 110, 120]])\n","\n","# Mostramos tamaños\n","print(\"Tamaño de tensor x: \", x.size())\n","print(\"Tamaño de tensor y: \", y.size())\n","\n","# Empilamos tensores a lo largo de la dimensión 0 \n","xy = torch.stack([x,y] , dim=0) # (3, 4) --> (1, 3, 4) --> (N, 3, 4)\n","print(\"\\nTensor empilado: \")\n","print(xy)\n","print(\"Tamaño de tensor xy: \", xy.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8JMw3TWIUwa","executionInfo":{"status":"ok","timestamp":1659558105868,"user_tz":300,"elapsed":83,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"8122015d-5cac-4faf-c437-617b0107e6b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño de tensor x:  torch.Size([3, 4])\n","Tamaño de tensor y:  torch.Size([3, 4])\n","\n","Tensor empilado: \n","tensor([[[  1,   2,   3,   4],\n","         [  5,   6,   7,   8],\n","         [  9,  10,  11,  12]],\n","\n","        [[ 10,  20,  30,  40],\n","         [ 50,  60,  70,  80],\n","         [ 90, 100, 110, 120]]])\n","Tamaño de tensor xy:  torch.Size([2, 3, 4])\n"]}]},{"cell_type":"markdown","source":["Podemos ver que el tamaño de la salida adiciona una nueva dimensión acorde a lo especificado en el método stack. Este proceso es lo mismo que realizar la operación de unsqueeze seguido de una concatenación. En el ejemplo vemos que los tensores que queremos empilar tienen la misma dimensión, $(3,4)$. Al hacer el empilado, primero se adiciona una dimensión en 0, $(1,3,4)$ y luego se concatenan los tensores resultando en un tensor de tamaño $(2,3,4)$.\n","\n","Entonces tenemos que:\n","\n","```python\n","torch.stack([x,y], dim) = torch.cat((torch.unsqueeze(x, dim), torch.unsqueeze(y,dim)), dim)\n","```\n","\n","Veamos el resultado:\n"],"metadata":{"id":"Ha1h-rPXN9my"}},{"cell_type":"code","source":["# Empilamos tensores a lo largo de la dimensión 0 \n","xy = torch.cat( (torch.unsqueeze(x, dim=0), torch.unsqueeze(y,dim=0)), dim=0)\n","print(\"\\nTensor empilado: \")\n","print(xy)\n","print(\"Tamaño de tensor xy: \", xy.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Bb1t0y3RjAK","executionInfo":{"status":"ok","timestamp":1659558105869,"user_tz":300,"elapsed":83,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"a2467aa9-bbe8-4405-dcf2-cc401ebf7737"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Tensor empilado: \n","tensor([[[  1,   2,   3,   4],\n","         [  5,   6,   7,   8],\n","         [  9,  10,  11,  12]],\n","\n","        [[ 10,  20,  30,  40],\n","         [ 50,  60,  70,  80],\n","         [ 90, 100, 110, 120]]])\n","Tamaño de tensor xy:  torch.Size([2, 3, 4])\n"]}]},{"cell_type":"markdown","source":["Vemos que el resultado es el mismo, corroborando lo afirmado. A continuación mostramos ejemplos empilando sobre otras dimensiones"],"metadata":{"id":"K7abjjTPRz3E"}},{"cell_type":"code","source":["# Empilamos tensores a lo largo de la dimensión 1\n","xy = torch.stack([x,y] , dim=1) # (3, 4) --> (3, 1, 4) --> (3, N, 4)\n","print(\"\\nTensor empilado en dimensión 1: \")\n","print(xy)\n","print(\"Tamaño de tensor xy: \", xy.size())\n","\n","# Empilamos tensores a lo largo de la dimensión 2\n","xy = torch.stack([x,y] , dim=2) # (3, 4) --> (3, 4, 1) --> (3, 4, N)\n","print(\"\\nTensor empilado en dimensión 2: \")\n","print(xy)\n","print(\"Tamaño de tensor xy: \", xy.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5UvjT0JR8pf","executionInfo":{"status":"ok","timestamp":1659558105870,"user_tz":300,"elapsed":81,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"9fee2c97-0498-449c-8c3f-769ab2b10e03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Tensor empilado en dimensión 1: \n","tensor([[[  1,   2,   3,   4],\n","         [ 10,  20,  30,  40]],\n","\n","        [[  5,   6,   7,   8],\n","         [ 50,  60,  70,  80]],\n","\n","        [[  9,  10,  11,  12],\n","         [ 90, 100, 110, 120]]])\n","Tamaño de tensor xy:  torch.Size([3, 2, 4])\n","\n","Tensor empilado en dimensión 2: \n","tensor([[[  1,  10],\n","         [  2,  20],\n","         [  3,  30],\n","         [  4,  40]],\n","\n","        [[  5,  50],\n","         [  6,  60],\n","         [  7,  70],\n","         [  8,  80]],\n","\n","        [[  9,  90],\n","         [ 10, 100],\n","         [ 11, 110],\n","         [ 12, 120]]])\n","Tamaño de tensor xy:  torch.Size([3, 4, 2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"oaPL4xI27yvL"},"source":["## Indexación\n","\n","A menudo tenemos la situación en la que necesitamos seleccionar una parte de un tensor. PyTorch proporciona muchas formas de indexar tensores. Sentirse cómodo con estas diferentes opciones facilita la modificación de diferentes partes de los tensores con facilidad."]},{"cell_type":"markdown","source":["### Tensor indexing\n","\n","La indexación de un tensor de Pytorch es similar a la de una lista de Python. La indexación del tensor pytorch se basa en 0, es decir, el primer elemento del arreglo tiene índice 0."],"metadata":{"id":"3y8lwrepixTm"}},{"cell_type":"code","source":["x = torch.tensor([1, 2, 3, 4, 5])\n"," \n","# Mostramos tensor\n","print(\"Tensor original:\", x)\n"," \n","# Accesamos valor usando índice válido\n","temp = x[2]\n","print(\"El valor de x[2] es:\", temp)\n"," \n","# Modificamos valor\n","x[2] = 10\n"," \n","# Mostramos tensor luego de modificar\n","print(\"Tensor después de modificar el valor:\", x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ta2L3UWIkC7y","executionInfo":{"status":"ok","timestamp":1659558105870,"user_tz":300,"elapsed":78,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"2b7424ad-2389-4271-cfcf-af73bd6b6534"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original: tensor([1, 2, 3, 4, 5])\n","El valor de x[2] es: tensor(3)\n","Tensor después de modificar el valor: tensor([ 1,  2, 10,  4,  5])\n"]}]},{"cell_type":"markdown","source":["Podemos acceder y modificar en varias dimensiones:"],"metadata":{"id":"K-404NFGk1iM"}},{"cell_type":"code","source":["x = torch.rand(size=(3,4,5)) # 3D tensor\n","\n","print('Tensor original:')\n","print(x)\n","print('\\n')\n","\n","# Formas de acceso válidas\n","print('x[0][0][0]\\n', x[0][0][0])\n","print('x[1,2,3]\\n', x[1,2,3])\n","print('x[-1,-1][-1]\\n', x[-1,-1][-1])\n","print('\\n')\n","\n","# Modificamos tensor\n","x[0][0][0] = -100\n","x[1,2,3] = 12.5\n","x[-1,-1,-1] = 10000.5\n","print('Tensor después de modificar sus valores:')\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PWwboDSk36p","executionInfo":{"status":"ok","timestamp":1659558105871,"user_tz":300,"elapsed":77,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}},"outputId":"7ccc2c81-c1ed-4d26-cb53-6b1fb996750e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original:\n","tensor([[[0.1427, 0.9851, 0.5783, 0.1433, 0.2137],\n","         [0.7490, 0.2377, 0.4277, 0.9045, 0.8610],\n","         [0.5942, 0.9265, 0.2299, 0.2243, 0.3835],\n","         [0.5560, 0.2684, 0.1374, 0.1094, 0.4948]],\n","\n","        [[0.1264, 0.5239, 0.8462, 0.4444, 0.3686],\n","         [0.9906, 0.4605, 0.6124, 0.9093, 0.3077],\n","         [0.2272, 0.2616, 0.1522, 0.0364, 0.1528],\n","         [0.4981, 0.0984, 0.0514, 0.7338, 0.9130]],\n","\n","        [[0.0015, 0.2598, 0.9701, 0.2717, 0.9355],\n","         [0.4000, 0.9253, 0.4605, 0.7021, 0.3879],\n","         [0.7867, 0.0265, 0.4429, 0.8799, 0.4947],\n","         [0.0391, 0.0644, 0.5005, 0.5835, 0.7942]]])\n","\n","\n","x[0][0][0]\n"," tensor(0.1427)\n","x[1,2,3]\n"," tensor(0.0364)\n","x[-1,-1][-1]\n"," tensor(0.7942)\n","\n","\n","Tensor después de modificar sus valores:\n","tensor([[[-1.0000e+02,  9.8513e-01,  5.7832e-01,  1.4335e-01,  2.1372e-01],\n","         [ 7.4898e-01,  2.3773e-01,  4.2769e-01,  9.0454e-01,  8.6102e-01],\n","         [ 5.9422e-01,  9.2648e-01,  2.2994e-01,  2.2428e-01,  3.8348e-01],\n","         [ 5.5604e-01,  2.6842e-01,  1.3744e-01,  1.0935e-01,  4.9476e-01]],\n","\n","        [[ 1.2639e-01,  5.2389e-01,  8.4622e-01,  4.4444e-01,  3.6858e-01],\n","         [ 9.9061e-01,  4.6051e-01,  6.1235e-01,  9.0930e-01,  3.0768e-01],\n","         [ 2.2724e-01,  2.6163e-01,  1.5216e-01,  1.2500e+01,  1.5276e-01],\n","         [ 4.9814e-01,  9.8376e-02,  5.1393e-02,  7.3382e-01,  9.1298e-01]],\n","\n","        [[ 1.5292e-03,  2.5983e-01,  9.7009e-01,  2.7171e-01,  9.3545e-01],\n","         [ 4.0003e-01,  9.2526e-01,  4.6046e-01,  7.0213e-01,  3.8788e-01],\n","         [ 7.8667e-01,  2.6501e-02,  4.4294e-01,  8.7992e-01,  4.9473e-01],\n","         [ 3.9061e-02,  6.4403e-02,  5.0052e-01,  5.8348e-01,  1.0000e+04]]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"NJQoIUlO3b3w"},"source":["### Slice indexing\n","\n","Similar a las listas de Python y las matrices numpy, los tensores de PyTorch se pueden **cortar (slice)** usando la sintaxis `start:stop` o` start:stop:step`. El índice `stop` es siempre no inclusivo: es el primer elemento que no se incluye en el segmento.\n","\n","Los índices de inicio y finalización pueden ser negativos, en cuyo caso se cuentan hacia atrás desde el final del tensor."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-wvWNwma365N","outputId":"cda92fed-3c82-4ee5-a618-d7248ad9c08c","executionInfo":{"status":"ok","timestamp":1659558105872,"user_tz":300,"elapsed":71,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["a = torch.tensor([0, 11, 22, 33, 44, 55, 66])\n","print(0, a)        # (0) Tensor original\n","print(1, a[2:5])   # (1) Elementos entre índice 2 y 5\n","print(2, a[2:])    # (2) Elementos a partir del índice 2\n","print(3, a[:5])    # (3) Elementos antes del índice 5\n","print(4, a[:])     # (4) Todos los elementos\n","print(5, a[1:5:2]) # (5) Cada segundo elemento entre los índices 1 y 5 (incluye el índice inicial 1)\n","print(6, a[:-1])   # (6) Todo menos el último elemento\n","print(7, a[-4::2]) # (7) Cada segundo elemento, empezando por el cuarto último elemento"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 tensor([ 0, 11, 22, 33, 44, 55, 66])\n","1 tensor([22, 33, 44])\n","2 tensor([22, 33, 44, 55, 66])\n","3 tensor([ 0, 11, 22, 33, 44])\n","4 tensor([ 0, 11, 22, 33, 44, 55, 66])\n","5 tensor([11, 33])\n","6 tensor([ 0, 11, 22, 33, 44, 55])\n","7 tensor([33, 55])\n"]}]},{"cell_type":"markdown","metadata":{"id":"eM5WifSk5t3B"},"source":["Para tensores multidimensionales, puede proporcionar un segmento o un número entero para cada dimensión del tensor con el fin de extraer diferentes tipos de subtensores:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gBtrwVL52WY","outputId":"a238fc6e-6171-4d01-f725-01fee0ee4501","executionInfo":{"status":"ok","timestamp":1659558105873,"user_tz":300,"elapsed":66,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Creamos el siguiente tensor de 2 dimensiones de tamaño (3, 4)\n","# [[ 1  2  3  4]\n","#  [ 5  6  7  8]\n","#  [ 9 10 11 12]]\n","a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n","print('Tensor original:')\n","print(a)\n","print('tamaño: ', a.shape)\n","\n","# Obtenemos la fila 1 y todas las columnas.\n","print('\\nUna fila:')\n","print(a[1,:])\n","print(a[1]) # Da el mismo resultado; podemos omitir: para dimensiones finales\n","print('tamaño:', a[1].shape)\n","\n","print('\\nColumna única:')\n","print(a[:, 1])\n","print('tamaño: ', a[:, 1].shape)\n","\n","# Obtenemos las dos primeras filas y las últimas tres columnas\n","print('\\nPrimeras dos filas, últimas tres columnas:')\n","print(a[:2, -3:])\n","print('tamaño: ', a[:2, -3:].shape)\n","\n","# Obtenemos cada dos filas y columnas en el índice 1 y 2\n","print('\\nCada dos fila, columnas del medio:')\n","print(a[::2, 1:3])\n","print('tamaño: ', a[::2, 1:3].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original:\n","tensor([[ 1,  2,  3,  4],\n","        [ 5,  6,  7,  8],\n","        [ 9, 10, 11, 12]])\n","tamaño:  torch.Size([3, 4])\n","\n","Una fila:\n","tensor([5, 6, 7, 8])\n","tensor([5, 6, 7, 8])\n","tamaño: torch.Size([4])\n","\n","Columna única:\n","tensor([ 2,  6, 10])\n","tamaño:  torch.Size([3])\n","\n","Primeras dos filas, últimas tres columnas:\n","tensor([[2, 3, 4],\n","        [6, 7, 8]])\n","tamaño:  torch.Size([2, 3])\n","\n","Cada dos fila, columnas del medio:\n","tensor([[ 2,  3],\n","        [10, 11]])\n","tamaño:  torch.Size([2, 2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"k944A6mP8Dl0"},"source":["Hay dos formas comunes de acceder a una sola fila o columna de un tensor: el uso de un número entero reducirá la dimensión en uno y el uso de un segmento de longitud uno mantendrá la misma dimensión. Tenga en cuenta que este es un comportamiento diferente al de MATLAB."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQnM7k5r8NJy","outputId":"79aca29d-f8a4-4998-c57f-181a9582f441","executionInfo":{"status":"ok","timestamp":1659558105874,"user_tz":300,"elapsed":61,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Creamos el siguiente tensor de dimensión 2 de tamaño (3, 4)\n","a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n","print('Tensor original')\n","print(a)\n","\n","row_r1 = a[1, :]    # Segunda fila del tensor 'a' de 1 dimensión\n","row_r2 = a[1:2, :]  # Segunda fila del tensor 'a' de 2 dimensiones\n","print('\\nDos formas de acceder a una sola fila:')\n","print(row_r1, row_r1.shape)\n","print(row_r2, row_r2.shape)\n","\n","# Podemos hacer la misma distinción al acceder a las columnas:\n","col_r1 = a[:, 1]\n","col_r2 = a[:, 1:2]\n","print('\\nDos formas de acceder a una sola columna:')\n","print(col_r1, col_r1.shape)\n","print(col_r2, col_r2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original\n","tensor([[ 1,  2,  3,  4],\n","        [ 5,  6,  7,  8],\n","        [ 9, 10, 11, 12]])\n","\n","Dos formas de acceder a una sola fila:\n","tensor([5, 6, 7, 8]) torch.Size([4])\n","tensor([[5, 6, 7, 8]]) torch.Size([1, 4])\n","\n","Dos formas de acceder a una sola columna:\n","tensor([ 2,  6, 10]) torch.Size([3])\n","tensor([[ 2],\n","        [ 6],\n","        [10]]) torch.Size([3, 1])\n"]}]},{"cell_type":"markdown","metadata":{"id":"NwRZbkDX_piW"},"source":["### Indexación de tensor de enteros\n","\n","Cuando indexamos el tensor usando slices, la vista del tensor resultante siempre será un subarreglo del tensor original. Esto es poderoso, pero puede ser restrictivo.\n","\n","También podemos usar **matrices de índice** para indexar tensores; esto nos permite construir nuevos tensores con mucha más flexibilidad que usando slices.\n","\n","Como ejemplo, podemos usar matrices de índices para reordenar las filas o columnas de un tensor:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"roR2VYGnAQuQ","outputId":"1f5b4bfd-def1-40e3-f0a0-1fe852f808ab","executionInfo":{"status":"ok","timestamp":1659558105875,"user_tz":300,"elapsed":56,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Creamos el siguiente tensor de dimensión 2 de tamaño (3, 4)\n","# [[ 1  2  3  4]\n","#  [ 5  6  7  8]\n","#  [ 9 10 11 12]]\n","a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n","print('Tensor original:')\n","print(a)\n","\n","# Creamos un nuevo tensor de tamaño (5, 4) reordenando las filas de a:\n","# - Las primeras dos filas son iguales a la primera fila del tensor 'a'\n","# - La tercera fila es igual que la última fila del tensor 'a'\n","# - La cuarta y quinta filas son iguales a la segunda fila del tensor 'a'\n","idx = [0, 0, 2, 1, 1]  # los índices pueden ser listas de números enteros de Python\n","print('\\nReordenamos filas:')\n","print(a[idx])\n","\n","# Creamos un nuevo tensor de tamaño (3, 4) reordenando las columnas de a:\n","idx = torch.tensor([3, 2, 1, 0])  # Las matrices de índices pueden ser tensores de antorcha int64\n","print('\\nReordenamos columnas:')\n","print(a[:, idx])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original:\n","tensor([[ 1,  2,  3,  4],\n","        [ 5,  6,  7,  8],\n","        [ 9, 10, 11, 12]])\n","\n","Reordenamos filas:\n","tensor([[ 1,  2,  3,  4],\n","        [ 1,  2,  3,  4],\n","        [ 9, 10, 11, 12],\n","        [ 5,  6,  7,  8],\n","        [ 5,  6,  7,  8]])\n","\n","Reordenamos columnas:\n","tensor([[ 4,  3,  2,  1],\n","        [ 8,  7,  6,  5],\n","        [12, 11, 10,  9]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"hqnDC9RhBXIb"},"source":["De manera más general, dadas las matrices de índices `idx0` e` idx1` con `N` elementos cada una,` a[idx0, idx1] `es equivalente a:\n","\n","```\n","torch.tensor ([\n","   a[idx0[0], idx1[0]],\n","   a[idx0[1], idx1[1]],\n","   ...,\n","   a[idx0[N - 1], idx1[N - 1]]\n","])\n","```\n","\n","(Un patrón similar se extiende a tensores con más de dos dimensiones)\n","\n","Podemos, por ejemplo, usar esto para obtener o establecer la diagonal de un tensor:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ttaP_g0BqiV","outputId":"d535d582-7337-4e10-ad07-b9e5ad358154","executionInfo":{"status":"ok","timestamp":1659558105876,"user_tz":300,"elapsed":52,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","print('Tensor original:')\n","print(a)\n","\n","idx = [0, 1, 2]\n","print('\\nObtener la diagonal:')\n","print(a[idx, idx])\n","\n","# Modificar la diagonal\n","a[idx, idx] = torch.tensor([11, 22, 33])\n","print('\\nDespués de establecer la diagonal:')\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original:\n","tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","\n","Obtener la diagonal:\n","tensor([1, 5, 9])\n","\n","Después de establecer la diagonal:\n","tensor([[11,  2,  3],\n","        [ 4, 22,  6],\n","        [ 7,  8, 33]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"qSFot2D3B4J4"},"source":["Un truco útil con la indexación de matrices de enteros es seleccionar o mutar un elemento de cada fila o columna de una matriz:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SP5MAIl3CANj","outputId":"96bd9d36-2b2e-4d10-968d-a31ce70ef8f2","executionInfo":{"status":"ok","timestamp":1659558105877,"user_tz":300,"elapsed":47,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Creamos un nuevo tensor del que seleccionaremos elementos\n","a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n","print('Tensor original:')\n","print(a)\n","\n","# Tomamos el elemento de cada fila de a:\n","# de la fila 0, tomamos el elemento 1;\n","# de la fila 1, tomamos el elemento 2;\n","# de la fila 2, tomamos el elemento 1;\n","# de la fila 3, tomamos el elemento 0\n","idx0 = torch.arange(a.shape[0])  # Manera rápida de construir [0, 1, 2, 3]\n","idx1 = torch.tensor([1, 2, 1, 0])\n","print('\\nSeleccionamos un elemento de cada fila:')\n","print(a[idx0, idx1])\n","\n","# Ahora establezca cada uno de esos elementos en cero\n","a[idx0, idx1] = 0\n","print('\\nDespués de modificar un elemento de cada fila:')\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original:\n","tensor([[ 1,  2,  3],\n","        [ 4,  5,  6],\n","        [ 7,  8,  9],\n","        [10, 11, 12]])\n","\n","Seleccionamos un elemento de cada fila:\n","tensor([ 2,  6,  8, 10])\n","\n","Después de modificar un elemento de cada fila:\n","tensor([[ 1,  0,  3],\n","        [ 4,  5,  0],\n","        [ 7,  0,  9],\n","        [ 0, 11, 12]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"W1yEIMrWCvos"},"source":["### Indexación de tensor booleano\n","\n","La indexación de tensor booleano le permite seleccionar elementos arbitrarios de un tensor de acuerdo con una máscara booleana. Con frecuencia, este tipo de indexación se utiliza para seleccionar o modificar los elementos de un tensor que satisfacen alguna condición.\n","\n","En PyTorch, usamos tensores de dtype `torch.bool` para contener máscaras booleanas.\n","\n","(Antes de la versión 1.2.0, no existía el tipo `torch.bool` por lo que en su lugar` torch.uint8` se usaba generalmente para representar datos booleanos, con 0 indicando falso y 1 indicando verdadero. ¡Cuidado con esto en el código PyTorch más antiguo! )"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hc_-pKXeDBU4","outputId":"41fb68ac-731e-4aaf-aa1c-2c62bd9695c5","executionInfo":{"status":"ok","timestamp":1659558105878,"user_tz":300,"elapsed":43,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["a = torch.tensor([[1,2], [3, 4], [5, 6]])\n","print('Tensor original:')\n","print(a)\n","\n","# Encontramos los elementos de 'a' que sean mayores que 3. La máscara tiene el \n","# mismo tamaño que 'a', donde cada elemento de la máscara indica si el elemento \n","# correspondiente de 'a' es mayor que tres\n","mask = (a > 3)\n","print('\\nTensor de máscara:')\n","print(mask)\n","\n","# Podemos usar la máscara para construir un tensor de 1 dimensi[on] que contenga \n","# los elementos de 'a' que son seleccionados por la máscara\n","print('\\nSeleccionamos elementos con la máscara:')\n","print(a[mask])\n","\n","# También podemos usar máscaras booleanas para modificar tensores; por ejemplo, \n","# esto establece todos los elementos <= 3 en cero:\n","a[a <= 3] = 0\n","print('\\nDespués de modificar con máscara:')\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor original:\n","tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n","\n","Tensor de máscara:\n","tensor([[False, False],\n","        [False,  True],\n","        [ True,  True]])\n","\n","Seleccionamos elementos con la máscara:\n","tensor([4, 5, 6])\n","\n","Después de modificar con máscara:\n","tensor([[0, 0],\n","        [0, 4],\n","        [5, 6]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"dZUUinTZHjhy"},"source":["## Broadcasting\n","\n","Broadcasting es un mecanismo poderoso que permite a PyTorch trabajar con matrices de diferentes formas al realizar operaciones aritméticas. Con frecuencia tenemos un tensor más pequeño y un tensor más grande, y queremos usar el tensor más pequeño varias veces para realizar alguna operación en el tensor más grande.\n","\n","Por ejemplo, suponga que queremos agregar un vector constante a cada fila de un tensor. Podríamos hacerlo así:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdpHA3s2H3LN","outputId":"dfc31ebd-3200-4bba-d747-7b5441e1219f","executionInfo":{"status":"ok","timestamp":1659558105879,"user_tz":300,"elapsed":39,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Agregaremos el vector v a cada fila de la matriz x, almacenando el resultado \n","# en la matriz y\n","x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n","v = torch.tensor([1, 0, 1])\n","y = torch.zeros_like(x)   # Crea una matriz vacía con el mismo tamaño que x\n","\n","# Sumamos el vector v a cada fila de la matriz x con un bucle explícito\n","for i in range(4):\n","    y[i, :] = x[i, :] + v\n","\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2,  2,  4],\n","        [ 5,  5,  7],\n","        [ 8,  8, 10],\n","        [11, 11, 13]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"-5n1ViJAIOzQ"},"source":["Esto funciona; sin embargo, cuando el tensor x es muy grande, calcular un bucle explícito en Python podría ser lento. Tenga en cuenta que sumar el vector v a cada fila del tensor x es equivalente a formar un tensor vv apilando múltiples copias de v verticalmente, luego realizando la suma de elementos de x y vv. Podríamos implementar este enfoque así:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eEf4iOpYIZK5","outputId":"81e07e6b-30c1-4a00-ec45-9c27c0119d39","executionInfo":{"status":"ok","timestamp":1659558105880,"user_tz":300,"elapsed":39,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["vv = v.repeat((4, 1))  # Apilamos 4 copias de v una encima de la otra\n","print(vv)             "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 0, 1],\n","        [1, 0, 1],\n","        [1, 0, 1],\n","        [1, 0, 1]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"zQoPMPBlIuNC"},"source":["Sumamos los tensores"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpdcR6AsIwl5","outputId":"9a9cc746-583a-438a-f814-84dae3f419af","executionInfo":{"status":"ok","timestamp":1659558105882,"user_tz":300,"elapsed":39,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["y = x + vv  # Sumar x y vv\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2,  2,  4],\n","        [ 5,  5,  7],\n","        [ 8,  8, 10],\n","        [11, 11, 13]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"uQ6vlvMjI4t6"},"source":["Broadcasting de PyTorch nos permite realizar este cálculo sin crear realmente múltiples copias de v. Considere esta versión, usando broadcasting:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36-CY4TXJKBH","outputId":"8b0ec5c0-a1f8-44bc-fab9-3cb46ed4cb7b","executionInfo":{"status":"ok","timestamp":1659558105883,"user_tz":300,"elapsed":38,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["# Agregaremos el vector v a cada fila de la matriz x almacenando \n","# el resultado en la matriz y\n","x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n","v = torch.tensor([1, 0, 1])\n","y = x + v  # Agregue v a cada fila de x usando broadcasting\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2,  2,  4],\n","        [ 5,  5,  7],\n","        [ 8,  8, 10],\n","        [11, 11, 13]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Tw_YSi_1Jf00"},"source":["La línea y = x + v funciona aunque x tiene forma (4, 3) y v tiene forma (3,) debido a broadcasting; esta línea funciona como si v realmente tuviera forma (4, 3), donde cada fila es una copia de v, y la suma se realiza por elementos.\n","\n","Broadcasting de dos tensores juntos sigue las siguientes reglas:\n","\n","![alt](https://jidindinesh.github.io/img/Capture.PNG)\n","\n","1. Si los tensores no tienen la misma dimensionalidad, anteponga 1 al tamaño de la matriz de dimensión inferior hasta que ambos tensores tengan las mismas dimensiones. A: (3 x 4 x 5), B: (4 x 5) -> B: (1 x 4 x 5). RES: (3 x 4 x 5) \n","2. Se dice que los dos tensores son *compatibles* en una dimensión si tienen el mismo tamaño en la dimensión, o si uno de los tensores tiene el tamaño 1 en esa dimensión. A: (1 x 4 x 1), B: (3 x 4 x 5)\n","3. Se puede hacer broadcasting entre dos tensores si son compatibles en todas las dimensiones.\n","4. Después de broadcasting, cada tensor se comporta como si tuviese una forma igual a la forma máxima de los elementos de los dos tensores de entrada.\n","5. En cualquier dimensión donde un tensor tiene un tamaño 1 y el otro tensor tiene un tamaño mayor que 1, el primer tensor se comporta como si se hubiera copiado a lo largo de esa dimensión. A: (1 x 4 x 1), B: (3 x 4 x 5). RES: (3 x 4 x 5)\n","\n","Si esta explicación no tiene sentido, intente leer la explicación de la [documentación](https://pytorch.org/docs/stable/notes/broadcasting.html).\n","\n","Broadcasting generalmente ocurre implícitamente dentro de muchos operadores de PyTorch. Sin embargo, también podemos realizarlo explícitamente usando la función [`torch.broadcast_tensors`](https://pytorch.org/docs/stable/generated/torch.broadcast_tensors.html#torch.broadcast_tensors)."]},{"cell_type":"markdown","metadata":{"id":"DiLqlKO4a_ZT"},"source":["## Soporte de GPU\n","\n","Una característica crucial de PyTorch es el soporte de GPU, abreviatura de Graphics Processing Unit. Una GPU puede realizar muchos miles de pequeñas operaciones en paralelo, lo que la hace muy adecuada para realizar grandes operaciones matriciales en redes neuronales.\n","\n","Las CPU y las GPU tienen diferentes ventajas y desventajas, por lo que muchas computadoras contienen ambos componentes y los usan para diferentes tareas. En caso de que no esté familiarizado con las GPU, puede leer más detalles en esta [publicación de blog de NVIDIA](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) o [aquí](https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html).\n","\n","Las GPU pueden acelerar el entrenamiento de una red hasta un factor de $100$ que es esencial para las redes neuronales profundas. PyTorch implementa muchas funciones para admitir GPU (principalmente las de NVIDIA debido a las bibliotecas [CUDA](https://developer.nvidia.com/cuda-zone) y [cuDNN](https://developer.nvidia.com/cudnn)). Primero, verifiquemos si tiene una GPU disponible: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OH9MfodHMAEQ","outputId":"b12dd42d-87e2-41b2-99d0-330b3232effe","executionInfo":{"status":"ok","timestamp":1659558105883,"user_tz":300,"elapsed":36,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["gpu = torch.cuda.is_available()\n","print(\"¿La GPU está disponible? %s\" % str(gpu))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["¿La GPU está disponible? True\n"]}]},{"cell_type":"markdown","metadata":{"id":"FpPVmSOdcv_v"},"source":["Si tiene una GPU en su computadora, pero el comando anterior devuelve False, asegúrese de tener instalada la versión de CUDA correcta. En Google Colab, asegúrese de haber seleccionado una GPU en su configuración de tiempo de ejecución (en el menú, marque en `Runtime -> Change runtime type`).\n","\n","De forma predeterminada, todos los tensores que crean se almacenan en la CPU. Podemos enviar un tensor a la GPU usando la función `.to(...)`, o `.cuda()`. Sin embargo, a menudo es una buena práctica definir un objeto \"device\" en su código que apunte a la GPU si tiene una, o en caso contrario a la CPU. Luego, puede escribir su código con respecto a este objeto de device, lo que permite ejecutar el mismo código tanto en un sistema solo con CPU como en uno con una GPU. Probémoslo a continuación. Podemos especificar el device de la siguiente manera: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9aXS2HRIco29","outputId":"9fefe2ad-552b-4b10-98fa-e4e7075fd4fc","executionInfo":{"status":"ok","timestamp":1659558105884,"user_tz":300,"elapsed":35,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(\"Device\", device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"6y8uqI5Cdj0R"},"source":["Ahora creemos un tensor y enviémoslo al device respectivo: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sp7hsDg0dftJ","outputId":"76817024-876c-4dbe-8008-072581baaac1","executionInfo":{"status":"ok","timestamp":1659558109624,"user_tz":300,"elapsed":3773,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.zeros(2, 3)\n","x = x.to(device) #también puede especificar el device como string \"gpu\" o \"cpu\"\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.]], device='cuda:0')\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1ZFa-Bkdpse","outputId":"a5a5a702-4154-4c7a-d694-d89545eea176","executionInfo":{"status":"ok","timestamp":1659558116686,"user_tz":300,"elapsed":7069,"user":{"displayName":"Jhosimar Arias","userId":"09976505046763269224"}}},"source":["x = torch.randn(5000, 5000)\n","\n","## versión CPU\n","start_time = time.time()\n","_ = torch.matmul(x, x)\n","end_time = time.time()\n","print(\"CPU time: %6.5fs\" % (end_time - start_time))\n","\n","## versión GPU\n","x = x.to(device)\n","\n","# La primera operación en CUDA puede ser lenta, ya que primero debe establecer una comunicación CPU-GPU.\n","#Por lo tanto, primero ejecutemos un comando arbitrario sin cronometrarlo para una comparación justa.\n","if torch.cuda.is_available():\n","    _ = torch.matmul(x*0.0, x)\n","    \n","start_time = time.time()\n","_ = torch.matmul(x, x)\n","end_time = time.time()\n","print(\"GPU time: %6.5fs\" % (end_time - start_time))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU time: 4.27140s\n","GPU time: 0.00013s\n"]}]},{"cell_type":"markdown","metadata":{"id":"qIv_u1Oeea2a"},"source":["Dependiendo del tamaño de la operación y la CPU/GPU en su sistema, la aceleración de esta operación puede ser > 500x. Como las operaciones `matmul` son muy comunes en las redes neuronales, ya podemos ver el gran beneficio de entrenar una red neuronal en una GPU. La estimación de tiempo puede ser relativamente ruidosa aquí porque no la hemos ejecutado varias veces. Siéntase libre de extender esto, pero también puede llevar más tiempo en ejecutarlo.\n","\n","Al generar números aleatorios, el `seed` entre CPU y GPU no está sincronizada. Por lo tanto, debemos establecer el `seed` en la GPU por separado para garantizar un código reproducible. Tenga en cuenta que debido a las diferentes arquitecturas de GPU, ejecutar el mismo código en diferentes GPU no garantiza los mismos números aleatorios. Aún así, no queremos que nuestro código nos dé una salida diferente cada vez que lo ejecutamos exactamente en el mismo hardware. Por lo tanto, también establecemos el `seed` en la GPU: "]},{"cell_type":"code","metadata":{"id":"FGXyD6Hve2tz"},"source":["# Las operaciones de GPU tienen un seed separado que también queremos establecer \n","if torch.cuda.is_available(): \n","    torch.cuda.manual_seed(42)\n","    torch.cuda.manual_seed_all(42)\n","    \n","# Además, algunas operaciones en una GPU se implementan estocásticamente para mayor eficiencia.\n","# Queremos asegurarnos de que todas las operaciones sean deterministas en la GPU (si se usa) para la reproducibilidad \n","torch.backends.cudnn.determinstic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0FXWnuTfT9C"},"source":["**Referencias**\n","\n","1. [Tutorial Oficial de PyTorch](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)"]}]}